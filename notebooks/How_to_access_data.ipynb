{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66114497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b454cf9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 991.0 kB/s eta 0:00:13\n",
      "      --------------------------------------- 0.3/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     - -------------------------------------- 0.5/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.7/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.9/12.8 MB 3.9 MB/s eta 0:00:04\n",
      "     --- ------------------------------------ 1.1/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.1/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.3/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.4/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.7/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.7/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.0/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.2/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.1/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.3/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.5/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 6.1/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.3/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.8/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.5/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.2/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 4.3 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.1/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.6/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 11.9/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.5 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4b2587",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[43msentence\u001b[49m)\n\u001b[0;32m      6\u001b[0m lemm_sentence \u001b[38;5;241m=\u001b[39m [token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemm_sentence)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sentence)\n",
    "\n",
    "lemm_sentence = [token.lemma_ for token in doc]\n",
    "\n",
    "' '.join(lemm_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "019647f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "puncts = string.punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tags = re.compile(r'<.*?>')\n",
    "remove_non_alpha = re.compile(r'\\W')\n",
    "spaces = re.compile(r'\\s+')\n",
    "not_alpha_numeric = re.compile(r'[^a-zA-Z0-9\\s]')\n",
    "urls = re.compile(r'http\\S+|www\\S+')\n",
    "remove_non_ascii = re.compile(r'[^\\x00-\\x7F]+')\n",
    "mentions = re.compile(r'[@#]\\w+')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        'J' : wordnet.ADJ, \n",
    "        'N' : wordnet.NOUN,\n",
    "        'V' : wordnet.VERB,\n",
    "        'R' : wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def cleaning(text, stemming = True):\n",
    "    text = text.lower()\n",
    "    text = tags.sub('', text)\n",
    "#     text = remove_notn_alpha.sub(, ' ', text) # we don't want to lose the numerical information\n",
    "    text = spaces.sub(' ', text)\n",
    "    text = not_alpha_numeric.sub(' ', text)\n",
    "    text = urls.sub('', text)\n",
    "#     text = re.sub(r'\\d+', '', text)\n",
    "    text = remove_non_ascii.sub('', text) # to remove non-ascii characters\n",
    "    text = mentions.sub('', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stemming\n",
    "    if stemming:\n",
    "        s = [word for word in tokens if word not in stop_words]\n",
    "        s = [ps.stem(word) for word in s]\n",
    "        \n",
    "    # Lemmatization\n",
    "    else:\n",
    "        doc = nlp(text)\n",
    "        s = [token.lemma_ for token in doc]\n",
    "        \n",
    "    return ' '.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da403078",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Banking system low-cost deposits could decline further, says SBI chairman Setty SBI’s Casa ratio stood at 40.7% as on 30 June, down from 42.9% from the same period last year. Its loan book by a robust 15.4% year-on-year, outpacing deposit growth of 8.2% in the June quarter. Mumbai: The share of low-cost deposits in the banking system could decline further and go below the levels seen before covid-19 on the back of efficient cash management by the government, the chairman of India’s largest lender SBI C.S. Setty said.' \n",
    "\n",
    "\n",
    "sample = cleaning(txt, stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fd989d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banking system low cost deposit could decline far   say sbi chairman setty sbi s casa ratio stand at 40 7   as on 30 june   down from 42 9   from the same period last year   its loan book by a robust 15 4   year on year   outpace deposit growth of 8 2   in the june quarter   mumbai   the share of low cost deposit in the banking system could decline far and go below the level see before covid 19 on the back of efficient cash management by the government   the chairman of india s large lender sbi c s   setty say'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f18dfc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 12502/12502 [07:27<00:00, 27.92it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = data['article'].progress_apply(lambda x : cleaning(x, stemming = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a65082",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_data.pkl', 'rb') as file1:\n",
    "    cleaned_data = pkl.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1369726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenized_data.pkl', 'rb') as file1:\n",
    "    tokenized_data = pkl.load(file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf659450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c4345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98df9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b6e7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8dd0d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "published_at       0\n",
       "uuid               0\n",
       "sentiment_score    0\n",
       "article            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a021785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 12502/12502 [03:50<00:00, 54.19it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(text):\n",
    "    data = [word for word in word_tokenize(text) if word not in stop_words]\n",
    "    return data\n",
    "\n",
    "df = data['article'].progress_apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a36e45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_vector(tokens, model):\n",
    "    word_vectors = [model.wv.get_vector(word) for word in tokens if word in model.wv.key_to_index]\n",
    "    \n",
    "    if len(word_vectors) > 0:\n",
    "        emb_vector = np.mean(word_vectors, axis = 0)\n",
    "    else:\n",
    "        emb_vector = np.zeros(model.vector_size)\n",
    "    return emb_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "489465fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(df, vector_size=20, workers=8, window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0ab4560",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_data = sent_to_vector(sample, cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73b2227c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15, -0.31,  0.16,  0.22,  0.22, -0.7 ,  0.38,  0.17, -0.25,\n",
       "        0.05,  0.27, -0.23, -0.01, -0.19, -0.21,  0.65,  0.67, -0.54,\n",
       "       -0.26, -0.84], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(cbow_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "[ 0.15, -0.31,  0.16,  0.22,  0.22, -0.7 ,  0.38,  0.17, -0.25, 0.05,  0.27, -0.23, -0.01, -0.19, -0.21,  0.65,  0.67, -0.54, -0.26, -0.84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91642b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f97642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffb06cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=100, ngram_range=(2, 2), stop_words=&#x27;english&#x27;,\n",
       "                sublinear_tf=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=100, ngram_range=(2, 2), stop_words=&#x27;english&#x27;,\n",
       "                sublinear_tf=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=100, ngram_range=(2, 2), stop_words='english',\n",
       "                sublinear_tf=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(sublinear_tf = True, ngram_range=(2, 2), max_features = 100, stop_words='english')\n",
    "tf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0c8b0f6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sensex nifty': 81,\n",
       " 'stock market': 86,\n",
       " 'tata consultancy': 88,\n",
       " 'consultancy service': 22,\n",
       " 'state bank': 85,\n",
       " 'bank india': 14,\n",
       " 'ultratech cement': 93,\n",
       " 'larsen toubro': 50,\n",
       " 'adani port': 4,\n",
       " 'bajaj finance': 7,\n",
       " 'mahindra mahindra': 55,\n",
       " 'reliance industry': 73,\n",
       " 'bharti airtel': 17,\n",
       " 'icici bank': 35,\n",
       " 'hdfc bank': 32,\n",
       " 'psu bank': 70,\n",
       " 'bank index': 13,\n",
       " 'tata motors': 90,\n",
       " 'bajaj finserv': 8,\n",
       " 'india sbi': 41,\n",
       " 'punjab national': 72,\n",
       " 'national bank': 61,\n",
       " 'cent state': 20,\n",
       " 'tata motor': 89,\n",
       " 'bank hdfc': 11,\n",
       " 'bse sensex': 18,\n",
       " 'asian paint': 5,\n",
       " 'hindustan unilever': 33,\n",
       " 'india bank': 39,\n",
       " 'rs 000': 76,\n",
       " '000 crore': 0,\n",
       " 'fix deposit': 30,\n",
       " 'senior citizen': 79,\n",
       " 'axis bank': 6,\n",
       " 'share price': 83,\n",
       " 'crore state': 26,\n",
       " 'include state': 36,\n",
       " 'life insurance': 52,\n",
       " 'union bank': 94,\n",
       " 'lender state': 51,\n",
       " 'bank state': 15,\n",
       " 'indusind bank': 45,\n",
       " 'public sector': 71,\n",
       " 'reserve bank': 74,\n",
       " 'country large': 24,\n",
       " 'large lender': 49,\n",
       " 'new delhi': 63,\n",
       " 'basis point': 16,\n",
       " 'bank icici': 12,\n",
       " 'india large': 40,\n",
       " 'canara bank': 19,\n",
       " 'sector bank': 77,\n",
       " 'indian bank': 43,\n",
       " 'kotak mahindra': 47,\n",
       " 'mahindra bank': 54,\n",
       " 'vodafone idea': 98,\n",
       " 'home loan': 34,\n",
       " 'lakh crore': 48,\n",
       " 'supreme court': 87,\n",
       " 'bank baroda': 10,\n",
       " 'tech mahindra': 92,\n",
       " 'india stock': 42,\n",
       " 'nifty bank': 64,\n",
       " 'bank bank': 9,\n",
       " 'yes bank': 99,\n",
       " 'central bank': 21,\n",
       " 'crore rs': 25,\n",
       " 'tata steel': 91,\n",
       " 'power grid': 69,\n",
       " 'gainer nifty': 31,\n",
       " 'index trade': 38,\n",
       " 'fd rate': 29,\n",
       " 'corporation india': 23,\n",
       " 'net profit': 62,\n",
       " 'market update': 59,\n",
       " 'adani enterprise': 2,\n",
       " 'live update': 53,\n",
       " 'share bse': 82,\n",
       " 'sensex close': 80,\n",
       " 'market valuation': 60,\n",
       " '30 share': 1,\n",
       " 'nse nifty': 66,\n",
       " 'value firm': 97,\n",
       " 'sector stock': 78,\n",
       " 'update nifty': 95,\n",
       " 'nse nifty50': 67,\n",
       " 'jsw steel': 46,\n",
       " 'market sector': 58,\n",
       " 'market fall': 56,\n",
       " 'fall 30': 28,\n",
       " 'nifty pack': 65,\n",
       " 'pack hand': 68,\n",
       " 'indice stock': 44,\n",
       " 'index close': 37,\n",
       " 'stand gainer': 84,\n",
       " 'market rise': 57,\n",
       " 'rise 30': 75,\n",
       " 'adani group': 3,\n",
       " 'electoral bond': 27,\n",
       " 'update state': 96}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01424a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.75, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.66, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(tf.transform([sample]).toarray(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[0.  , 0.  , 0.  ,...... , 0.75, 0.  , 0.  ,... , 0.  , 0.66, 0.  , 0. , 0, ......]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2aaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95bfc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Embeddings: 100%|███████████████████████████████████████████████████| 20396/20396 [00:00<00:00, 174200.74it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}    \n",
    "    with open(file_path, 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def create_embedding_matrix(tokenizer, embeddings_index, embedding_dim = 300):\n",
    "    vocabulary_size = len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "    \n",
    "    for word, i in tqdm(tokenizer.word_index.items(), desc = 'Creating Embeddings'):\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def prepare_sequences(tokenizer, texts, max_length):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_length)\n",
    "    return padded_sequences\n",
    "\n",
    "glove_file_path = 'D:/DATA (D)/Glove/glove.6B/glove.6B.300d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "train_texts = cleaned_data\n",
    "\n",
    "tokenize = Tokenizer()\n",
    "tokenize.fit_on_texts(train_texts)\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(tokenize, glove_embeddings, embedding_dim = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e124dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "glove_data = prepare_sequences(tokenize, sample, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6246d762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0, 1731],\n",
       "       [   0,    0,    0, ...,    0,    0,   10],\n",
       "       [   0,    0,    0, ...,    0,    0, 1506],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0,    0,   18],\n",
       "       [   0,    0,    0, ...,    0,    0,   10],\n",
       "       [   0,    0,    0, ...,    0,    0, 1062]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8838501c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.35618508e-01  4.56813686e-02 -1.95589401e-02 -6.38279617e-02\n",
      " -3.28229629e-02  2.17241012e-02  2.29437966e-02  8.63058865e-02\n",
      " -3.18159871e-02 -1.44604933e+00  2.47517647e-03  9.91021022e-02\n",
      "  6.75166165e-03  2.80945282e-02  6.51413351e-02  7.65789673e-02\n",
      " -5.98912202e-02 -3.99680063e-02 -5.86587116e-02 -1.73704758e-01\n",
      " -1.16417602e-01  3.62021364e-02  1.66413546e-01  8.97203237e-02\n",
      " -6.61192760e-02  3.10124122e-02  4.79388572e-02 -8.11925456e-02\n",
      " -1.65717408e-01 -3.12348418e-02 -1.58275999e-02  2.18933672e-01\n",
      " -7.41436705e-02  1.78346649e-01 -7.21798122e-01 -9.14353058e-02\n",
      "  7.89156333e-02  7.15264454e-02 -4.06631045e-02 -1.42471604e-02\n",
      "  1.24502657e-02 -9.78958085e-02 -1.71315238e-01  2.46180639e-01\n",
      "  1.83217339e-02 -7.48973386e-03  2.11197920e-02  2.33086988e-01\n",
      " -1.72564805e-01 -3.23594734e-03  3.67420465e-02 -9.70273539e-02\n",
      " -4.87702601e-02 -2.65695956e-02  1.00972818e-03  2.94278488e-02\n",
      " -2.36095600e-02  1.33322984e-01  3.13311326e-03 -8.37082341e-02\n",
      "  2.21162140e-02  2.10166350e-02  2.20894232e-01 -2.01589108e-01\n",
      "  1.42822620e-02 -3.41977060e-01  8.61642659e-02 -1.72397066e-02\n",
      " -7.90709481e-02 -1.59384795e-02  4.59569022e-02  2.59908885e-01\n",
      "  7.82080069e-02 -3.13212015e-02 -9.57068801e-02 -4.79999790e-03\n",
      "  9.53363031e-02 -7.18051270e-02 -1.07865021e-01 -7.54252216e-03\n",
      " -1.12621561e-01 -1.06329612e-01  1.76820904e-02  8.81532654e-02\n",
      "  2.01088488e-01  9.57012549e-02 -1.48320839e-01  9.72605571e-02\n",
      " -1.08806947e-02  9.19564739e-02  2.97043398e-02  6.64485693e-02\n",
      " -2.44766071e-01 -9.94050205e-02  3.12137976e-02 -1.91375930e-02\n",
      " -3.08693230e-01  1.10969238e-01  2.70036664e-02 -2.45852128e-01\n",
      "  2.95159798e-02  9.42760035e-02  2.38387752e-02 -9.32903811e-02\n",
      " -1.10212758e-01 -1.66740656e-01  8.22081640e-02  1.05327919e-01\n",
      " -1.19787961e-01  1.47785738e-01 -6.19163252e-02 -3.44951123e-01\n",
      " -1.18377693e-01 -1.40197515e-01  7.06399679e-02  3.28388363e-02\n",
      "  7.48357102e-02  8.16356167e-02  2.82561909e-02 -2.49942407e-01\n",
      " -2.73048431e-02 -1.37526423e-01  5.21711856e-02  1.20372184e-01\n",
      "  6.53464766e-03  2.18619127e-02  2.07985900e-02 -1.81348659e-02\n",
      "  4.24283035e-02  3.27101164e-02 -7.56090283e-02  2.46730074e-01\n",
      "  4.56569577e-03 -7.05559552e-03  5.86808436e-02  8.64433423e-02\n",
      "  4.08985354e-02  3.41755189e-02  4.31905594e-03  1.13882825e-01\n",
      " -6.99473992e-02  5.27087785e-02  3.26838009e-02  1.05973296e-01\n",
      " -3.15050900e-01  2.86909677e-02 -5.54837137e-02  3.71455401e-02\n",
      " -9.12640616e-03  2.13211440e-02  4.73370194e-01 -7.94094875e-02\n",
      "  7.46155828e-02 -4.71050516e-02  3.15800458e-01  8.14886987e-02\n",
      " -1.56070992e-01  5.92191033e-02 -1.14782006e-01 -9.96186659e-02\n",
      " -2.59119254e-02 -7.63670634e-03 -5.97861148e-02  5.83841174e-04\n",
      " -1.85684729e-02  9.18465033e-02  1.11753702e-01 -2.24928651e-02\n",
      "  4.59812880e-02  2.50135779e-01  7.06968829e-02  1.02291048e-01\n",
      " -3.88403833e-01  8.22773799e-02 -1.13883659e-01 -5.35920933e-02\n",
      "  3.12483162e-02  8.27052519e-02 -1.59482062e-02  4.88577932e-01\n",
      "  7.87751451e-02  3.51877399e-02  6.42246455e-02  5.86749092e-02\n",
      "  2.21341494e-02 -5.39007522e-02 -8.18099454e-02  6.77087083e-02\n",
      " -3.46739334e-03  2.20914826e-01  2.19431240e-02  2.45869190e-01\n",
      " -3.07154562e-02 -7.14344904e-02 -2.79607978e-02 -1.80654019e-01\n",
      "  9.97488275e-02 -8.50560814e-02  8.35614130e-02 -3.26086208e-02\n",
      "  8.22258472e-01  3.54401534e-04 -1.60877723e-02  7.17077181e-02\n",
      " -6.72670230e-02  2.98996046e-02 -2.18107224e-01  4.06324565e-02\n",
      " -3.19920070e-02 -1.50290608e-01  1.51021993e-02 -6.41633570e-02\n",
      "  2.64192522e-02 -7.71813095e-02  4.77165617e-02  1.17284462e-01\n",
      " -7.42951035e-02  1.39988646e-01 -6.42103627e-02 -3.58028598e-02\n",
      "  1.35350704e-01 -6.94460422e-02 -1.92174464e-01  6.12791926e-02\n",
      " -5.87689206e-02 -4.39641951e-03 -2.54744347e-02 -1.53697776e-02\n",
      "  7.65009299e-02  1.55038768e-02 -7.20790327e-02  3.46574038e-02\n",
      " -5.91317452e-02  4.01926972e-02  1.82580471e-01  1.00109801e-01\n",
      "  9.74368155e-02  2.27885954e-02  1.14245936e-01  4.85927388e-02\n",
      " -8.64474028e-02 -2.31852010e-02  3.55726391e-01  5.84105179e-02\n",
      " -5.90990663e-01 -1.41524794e-02  2.58985192e-01 -2.26205394e-01\n",
      " -4.66881096e-02 -5.43704582e-03  1.30136488e-02 -3.08106244e-02\n",
      "  7.40467012e-02 -8.44490081e-02  8.58442932e-02 -5.78119978e-02\n",
      " -1.45764276e-01  4.30822931e-02  1.01746030e-01  1.14786826e-01\n",
      "  3.27088162e-02  4.00552247e-03  1.11934822e-03  5.58579937e-02\n",
      " -1.15734953e-02 -2.10311878e-02  1.10812463e-01  4.33363114e-03\n",
      "  6.11147769e-02  8.80016461e-02 -8.25213268e-04 -5.98065890e-02\n",
      " -1.94592262e-03  1.13454938e-01  2.79197562e-02  1.65184792e-02\n",
      " -1.56460488e+00 -1.16698287e-01  2.51938552e-01  9.91057754e-02\n",
      " -1.77665904e-01 -5.75180463e-02 -6.65811822e-02 -6.02810308e-02\n",
      " -6.14867546e-02  1.88114151e-01 -8.26170519e-02 -2.42195260e-02\n",
      "  1.98065061e-02  1.12074418e-02 -3.64236720e-02 -9.51379761e-02\n",
      " -7.05143213e-02  2.37157978e-02  1.82827383e-01  4.60761309e-01\n",
      " -1.97431557e-02 -3.25862408e-01 -9.91016477e-02 -9.32311174e-03]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "# Function to preprocess and get sentence embedding\n",
    "def get_sentence_embedding(sentence, embeddings_index, embedding_dim=300):\n",
    "    words = sentence.lower().split()  # Simple tokenization\n",
    "    valid_embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in embeddings_index:\n",
    "            valid_embeddings.append(embeddings_index[word])\n",
    "    \n",
    "    if valid_embeddings:\n",
    "        sentence_embedding = np.mean(valid_embeddings, axis=0)  # Averaging word embeddings\n",
    "    else:\n",
    "        sentence_embedding = np.zeros(embedding_dim)  # If no valid words, return zero vector\n",
    "\n",
    "    return sentence_embedding\n",
    "\n",
    "glove_file_path = 'D:/DATA (D)/Glove/glove.6B/glove.6B.300d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Example usage\n",
    "sentence = sample\n",
    "embedding = get_sentence_embedding(sentence, glove_embeddings)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcefa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14,  0.05, -0.02, -0.06, -0.03,  0.02,  0.02,  0.09, -0.03,\n",
       "       -1.45,  0.  ,  0.1 ,  0.01,  0.03,  0.07,  0.08, -0.06, -0.04,\n",
       "       -0.06, -0.17, -0.12,  0.04,  0.17,  0.09, -0.07,  0.03,  0.05,\n",
       "       -0.08, -0.17, -0.03, -0.02,  0.22, -0.07,  0.18, -0.72, -0.09,\n",
       "        0.08,  0.07, -0.04, -0.01,  0.01, -0.1 , -0.17,  0.25,  0.02,\n",
       "       -0.01,  0.02,  0.23, -0.17, -0.  ,  0.04, -0.1 , -0.05, -0.03,\n",
       "        0.  ,  0.03, -0.02,  0.13,  0.  , -0.08,  0.02,  0.02,  0.22,\n",
       "       -0.2 ,  0.01, -0.34,  0.09, -0.02, -0.08, -0.02,  0.05,  0.26,\n",
       "        0.08, -0.03, -0.1 , -0.  ,  0.1 , -0.07, -0.11, -0.01, -0.11,\n",
       "       -0.11,  0.02,  0.09,  0.2 ,  0.1 , -0.15,  0.1 , -0.01,  0.09,\n",
       "        0.03,  0.07, -0.24, -0.1 ,  0.03, -0.02, -0.31,  0.11,  0.03,\n",
       "       -0.25,  0.03,  0.09,  0.02, -0.09, -0.11, -0.17,  0.08,  0.11,\n",
       "       -0.12,  0.15, -0.06, -0.34, -0.12, -0.14,  0.07,  0.03,  0.07,\n",
       "        0.08,  0.03, -0.25, -0.03, -0.14,  0.05,  0.12,  0.01,  0.02,\n",
       "        0.02, -0.02,  0.04,  0.03, -0.08,  0.25,  0.  , -0.01,  0.06,\n",
       "        0.09,  0.04,  0.03,  0.  ,  0.11, -0.07,  0.05,  0.03,  0.11,\n",
       "       -0.32,  0.03, -0.06,  0.04, -0.01,  0.02,  0.47, -0.08,  0.07,\n",
       "       -0.05,  0.32,  0.08, -0.16,  0.06, -0.11, -0.1 , -0.03, -0.01,\n",
       "       -0.06,  0.  , -0.02,  0.09,  0.11, -0.02,  0.05,  0.25,  0.07,\n",
       "        0.1 , -0.39,  0.08, -0.11, -0.05,  0.03,  0.08, -0.02,  0.49,\n",
       "        0.08,  0.04,  0.06,  0.06,  0.02, -0.05, -0.08,  0.07, -0.  ,\n",
       "        0.22,  0.02,  0.25, -0.03, -0.07, -0.03, -0.18,  0.1 , -0.09,\n",
       "        0.08, -0.03,  0.82,  0.  , -0.02,  0.07, -0.07,  0.03, -0.22,\n",
       "        0.04, -0.03, -0.15,  0.02, -0.06,  0.03, -0.08,  0.05,  0.12,\n",
       "       -0.07,  0.14, -0.06, -0.04,  0.14, -0.07, -0.19,  0.06, -0.06,\n",
       "       -0.  , -0.03, -0.02,  0.08,  0.02, -0.07,  0.03, -0.06,  0.04,\n",
       "        0.18,  0.1 ,  0.1 ,  0.02,  0.11,  0.05, -0.09, -0.02,  0.36,\n",
       "        0.06, -0.59, -0.01,  0.26, -0.23, -0.05, -0.01,  0.01, -0.03,\n",
       "        0.07, -0.08,  0.09, -0.06, -0.15,  0.04,  0.1 ,  0.11,  0.03,\n",
       "        0.  ,  0.  ,  0.06, -0.01, -0.02,  0.11,  0.  ,  0.06,  0.09,\n",
       "       -0.  , -0.06, -0.  ,  0.11,  0.03,  0.02, -1.56, -0.12,  0.25,\n",
       "        0.1 , -0.18, -0.06, -0.07, -0.06, -0.06,  0.19, -0.08, -0.02,\n",
       "        0.02,  0.01, -0.04, -0.1 , -0.07,  0.02,  0.18,  0.46, -0.02,\n",
       "       -0.33, -0.1 , -0.01], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(embedding, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e067169",
   "metadata": {},
   "outputs": [],
   "source": [
    "[-0.14,  0.05, -0.02, ......, -0.25,  0.03,  0.09,  0.02,...., 0.46, -0.02,-0.33, -0.1 , -0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5bd5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7dd23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting word from 'sbi - bank + investor': bankrupt\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Function to perform vector arithmetic\n",
    "def vector_arithmetic(word1, word2, word3, embeddings_index):\n",
    "    if word1 not in embeddings_index:\n",
    "        print('not', word1)\n",
    "    if word2 not in embeddings_index:\n",
    "        print('not', word2)\n",
    "    if word3 not in embeddings_index:\n",
    "        print('not', word3)\n",
    "    \n",
    "#     if word1 not in embeddings_index or word2 not in embeddings_index or word3 not in embeddings_index:\n",
    "#         print(\"One of the words is not in the vocabulary.\")\n",
    "        return None\n",
    "    \n",
    "    # Perform the vector arithmetic: word1 - word2 + word3\n",
    "    result_vector = embeddings_index[word1] - embeddings_index[word2] + embeddings_index[word3]\n",
    "    \n",
    "    return result_vector\n",
    "\n",
    "# Function to find the most similar word to a given vector\n",
    "def find_closest_word(vector, embeddings_index):\n",
    "    closest_word = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    for word, embedding in embeddings_index.items():\n",
    "        distance = cosine(vector, embedding)\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_word = word\n",
    "    \n",
    "    return closest_word\n",
    "\n",
    "# Example usage\n",
    "word1 = 'bankrupt'\n",
    "word2 = 'loss'\n",
    "word3 = 'benefit'\n",
    "\n",
    "# Ensure GloVe embeddings are loaded\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Perform vector arithmetic: sbi - bank + investor\n",
    "result_vector = vector_arithmetic(word1, word2, word3, glove_embeddings)\n",
    "\n",
    "# Find the closest word to the resulting vector\n",
    "if result_vector is not None:\n",
    "    closest_word = find_closest_word(result_vector, glove_embeddings)\n",
    "    print(\"Resulting word from 'sbi - bank + investor':\", closest_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38342995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35082198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest 3 words from 'sbi - bank + investor': ['bankrupt', 'benefit', 'financially']\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "import heapq\n",
    "\n",
    "# Function to perform vector arithmetic\n",
    "def vector_arithmetic(word1, word2, word3, embeddings_index):\n",
    "    if word1 not in embeddings_index or word2 not in embeddings_index or word3 not in embeddings_index:\n",
    "        print(\"One of the words is not in the vocabulary.\")\n",
    "        return None\n",
    "    \n",
    "    # Perform the vector arithmetic: word1 - word2 + word3\n",
    "    result_vector = embeddings_index[word1] - embeddings_index[word2] + embeddings_index[word3]\n",
    "    \n",
    "    return result_vector\n",
    "\n",
    "# Function to find the closest n words to a given vector\n",
    "def find_closest_words(vector, embeddings_index, n=3):\n",
    "    word_distances = []\n",
    "    \n",
    "    # Calculate cosine distance for each word in embeddings_index\n",
    "    for word, embedding in embeddings_index.items():\n",
    "        distance = cosine(vector, embedding)\n",
    "        word_distances.append((distance, word))\n",
    "    \n",
    "    # Get the n closest words by sorting by distance\n",
    "    closest_words = heapq.nsmallest(n, word_distances, key=lambda x: x[0])\n",
    "    \n",
    "    return [word for _, word in closest_words]\n",
    "\n",
    "# Example usage\n",
    "word1 = 'bankrupt'\n",
    "word2 = 'loss'\n",
    "word3 = 'benefit'\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Perform vector arithmetic: sbi - bank + investor\n",
    "result_vector = vector_arithmetic(word1, word2, word3, glove_embeddings)\n",
    "\n",
    "# Find the closest 3 words to the resulting vector\n",
    "if result_vector is not None:\n",
    "    closest_words = find_closest_words(result_vector, glove_embeddings, n=3)\n",
    "    print(\"Closest 3 words from 'sbi - bank + investor':\", closest_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1d189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15501f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f383f21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7a0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a42cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ec38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d7a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e4d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb0005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7b157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
